{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPS - Mercari Price ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:395: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Data/train.tsv\", delimiter=\"\\t\", index_col=0)\n",
    "\n",
    "# remove items with out a price\n",
    "data = data[pd.notna(data[\"price\"])]\n",
    "\n",
    "data[\"item_description\"] = data[\"item_description\"].replace(\"No description yet\", \"\")\n",
    "data[\"item_description\"] = data[\"item_description\"].replace(np.nan, \"\")\n",
    "\n",
    "temp = data[\"category_name\"].fillna('').str.split('/')\n",
    "              \n",
    "data[\"category_name_1\"] = temp.str[0]\n",
    "data[\"category_name_2\"] = temp.str[1]\n",
    "data[\"category_name_3\"] = temp.str[2:].str.join(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement porter stemming in count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "\n",
    "class StemmerTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "        self.translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(w) for w in doc\n",
    "                .encode('ascii', errors='ignore')\n",
    "                .decode('ascii')\n",
    "                .translate(self.translator)\n",
    "                .split()]\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase = True,\n",
    "                             max_df = .5,\n",
    "                             min_df = .001,\n",
    "                             tokenizer = StemmerTokenizer(),\n",
    "                             stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfm = vectorizer.fit_transform(data[\"item_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.vocabulary_ \n",
    "removed_words = vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "tfidf_transformed = tfidf_vectorizer.fit_transform(tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results for time savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"tfm.npz\", tfm)\n",
    "save_npz(\"tfidf_transformed.npz\", tfidf_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load files as necessary for time savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "tfm = load_npz(\"Data/tfm.npz\")\n",
    "tfidf_transformed = load_npz(\"Data/tfidf_transformed.npz\")\n",
    "cleaned_categorical = pd.read_csv('Data/train_clean.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482535, 1793)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482535, 1793)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1482535, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab: write vectorized words to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "with open(\"vocabulary.txt\", \"w\") as f:\n",
    "    f.write(\"\".join([k + '\\n' for k, v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y_categorical = cleaned_categorical['price'].astype('float', copy=False)\n",
    "cleaned_categorical.drop([\"train_id\",\"name\",\"category_name\",\"item_description\",\"price\"], axis=1, inplace=True)\n",
    "cleaned_categorical[\"item_condition_id\"] = cleaned_categorical[\"item_condition_id\"].astype('str', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe_cleaned_categorical = pd.get_dummies(cleaned_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test Train split\n",
    "y_cat_train, y_cat_test, X_cat_train, X_cat_test, tfidf_train, tfidf_test, tfm_train, tfm_test = train_test_split(\n",
    "    y_categorical, ohe_cleaned_categorical, tfidf_transformed, tfm, test_size=0.2, random_state=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the data to test models using train_test_split function\n",
    "yTrain, yTest, XTrain, XTest, tfidfTrain, tfidfTest, tfmTrain, tfmTest = train_test_split(\n",
    "    y_cat_test, X_cat_test, tfidf_test, tfm_test, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Category (non-text) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "lasso = LassoCV(cv=5, n_jobs=-1).fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yPred = lasso.predict(XTrain)\n",
    "residuals = yTrain - yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save lasso on category data as DataFrame, write to csv\n",
    "lasso_cat_coef = pd.DataFrame(lasso.coef_, index=XTrain.columns)\n",
    "lasso_cat_coef.to_csv(\"lasso_cat_coef.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Text data on residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.22 s, sys: 30 ms, total: 7.25 s\n",
      "Wall time: 7.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lassoTF = Lasso(alpha=alpha, random_state=10)\n",
    "lassoTF.fit(tfidfTrain, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('Data/vocabulary.txt', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save lasso on TFIDF data as DataFrame, write to csv\n",
    "lasso_tfidf_coef = pd.DataFrame(lassoTF.coef_, index=vocab.iloc[:,0].values)\n",
    "lasso_tfidf_coef.to_csv(\"lasso_tfidf_coef.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Text data on residuals (per category1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the category1 column headers\n",
    "cat1_cols = XTrain.iloc[:,4815:4825].columns\n",
    "\n",
    "#reset index so can filter using list of indices\n",
    "residuals.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.8 s, sys: 10.2 s, total: 1min 9s\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cat1_coefs = []\n",
    "tfidfTrain_df = pd.DataFrame(tfidfTrain.todense())  #I think can remove this and uncomment tocsr() line below\n",
    "\n",
    "for cat1 in cat1_cols:\n",
    "    #get indices where this category=1\n",
    "    idx = np.where(XTrain[cat1] == 1)[0]\n",
    "   \n",
    "    #filter data with the indices\n",
    "    X_tf = tfidfTrain_df.iloc[idx,:]   #Don't think need to use this, uncomment next time to deal with sparse\n",
    "    #X_tf = tfidfTrain.tocsr()[idx,:]  #way to filter sparse matrx with row indices\n",
    "    y_tf = residuals[idx]\n",
    "    \n",
    "    #run Lasso, fit, store coef array\n",
    "    lassoCat = Lasso(alpha=alpha, random_state=10)\n",
    "    lassoCat.fit(X_tf, y_tf)\n",
    "\n",
    "    #add this cat1 coefs to a list\n",
    "    cat1_coefs.append(lassoCat.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covert to DF with cols=vocab and index=category1 names\n",
    "lasso_allCat1_coef = pd.DataFrame(cat1_coefs, columns=vocab.iloc[:,0].values, index=cat1_cols)\n",
    "lasso_allCat1_coef.to_csv(\"lass_allCat1_coef.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><br>\n",
    "\n",
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
    "from keras.layers.recurrent import LSTM, RNN, GRU, SimpleRNN\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#default layers = 6, neurons list always should equal 6\n",
    "def build_NN(inShape, nnType='RNN'):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(inShape[1], 64, input_length=maxLen))  #input layer\n",
    "    if nnType == 'RNN':\n",
    "        model.add(SimpleRNN(32, return_sequences=True))\n",
    "        model.add(SimpleRNN(32))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "    elif nnType == 'GRU':\n",
    "        #model.add(Dense(64, input_shape=(inShape[1])))  #input layer\n",
    "        model.add(GRU(32, return_sequences=True))\n",
    "        model.add(GRU(32))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "    elif nnType == 'LSTM':\n",
    "        #model.add(Dense(64, input_shape=(inShape[1])))  #input layer\n",
    "        model.add(LSTM(32, return_sequences=True)) \n",
    "        model.add(LSTM(32))\n",
    "        model.add(Dense(16, activation='relu'))      \n",
    "        model.add(Dense(8, activation='relu'))\n",
    "    elif nnType == 'BASE':\n",
    "        #model.add(Dense(64, input_shape=(inShape[1],)))  #input layer\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Reshape((-1,)))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "    else:\n",
    "        print('should not be here')\n",
    "    \n",
    "    model.add(Dense(1, activation='linear', kernel_initializer=\"uniform\")) #output layer\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary() \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#params for NN\n",
    "epochs = 10\n",
    "batchSize = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to prep the data for the NN\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train5k)\n",
    "data = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1793\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 1793, 64)          128       \n",
      "_________________________________________________________________\n",
      "simple_rnn_13 (SimpleRNN)    (None, 1793, 32)          3104      \n",
      "_________________________________________________________________\n",
      "simple_rnn_14 (SimpleRNN)    (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 5,985\n",
      "Trainable params: 5,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelRNN = build_NN(tfidf_train.shape, nnType='RNN')\n",
    "modelRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelRNN.fit(tfidf_train, y_cat_train, batch_size=batchSize, epochs=epochs, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
