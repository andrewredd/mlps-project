{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPS - Mercari Price ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/train.tsv\", delimiter=\"\\t\", index_col=0)\n",
    "\n",
    "# remove items with out a price\n",
    "data = data[pd.notna(data[\"price\"])]\n",
    "\n",
    "data[\"item_description\"] = data[\"item_description\"].replace(\"No description yet\", \"\")\n",
    "data[\"item_description\"] = data[\"item_description\"].replace(np.nan, \"\")\n",
    "\n",
    "temp = data[\"category_name\"].fillna('').str.split('/')\n",
    "              \n",
    "data[\"category_name_1\"] = temp.str[0]\n",
    "data[\"category_name_2\"] = temp.str[1]\n",
    "data[\"category_name_3\"] = temp.str[2:].str.join(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement porter stemming in count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "\n",
    "class StemmerTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "        self.translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(w) for w in doc\n",
    "                .encode('ascii', errors='ignore')\n",
    "                .decode('ascii')\n",
    "                .translate(self.translator)\n",
    "                .split()]\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase = True,\n",
    "                             max_df = .5,\n",
    "                             min_df = .001,\n",
    "                             tokenizer = StemmerTokenizer(),\n",
    "                             stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tfm = vectorizer.fit_transform(data[\"item_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_ \n",
    "removed_words = vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "tfidf_transformed = tfidf_vectorizer.fit_transform(tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results for time savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"tfm.npz\", tfm)\n",
    "save_npz(\"tfidf_transformed.npz\", tfidf_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab: write vectorized words to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "with open(\"vocabulary.txt\", \"w\") as f:\n",
    "    f.write(\"\".join([k + '\\n' for k, v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><hr><hr>\n",
    "\n",
    "# Load files as necessary for time savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "tfm = load_npz(\"Data/tfm.npz\")\n",
    "tfidf_transformed = load_npz(\"Data/tfidf_transformed.npz\")\n",
    "categorical = pd.read_csv('Data/train_clean.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-filtering  1482535\n",
      "post-filtering  1478823\n"
     ]
    }
   ],
   "source": [
    "# take the log of price (0.000000001 added for item with 0 as price)\n",
    "categorical['log(price)'] = np.log(categorical['price']+ 0.000000001)\n",
    "\n",
    "# calculate 6 st dev of log(price)\n",
    "st_dev_log_price_6 = categorical['log(price)'].std()*6\n",
    "\n",
    "print(\"pre-filtering \",len(categorical))\n",
    "\n",
    "# find points that are not outliers\n",
    "not_outliers = np.array((categorical['log(price)'] < st_dev_log_price_6) & (categorical['log(price)'] > (- st_dev_log_price_6)))\n",
    "\n",
    "# filter dataset to data only around 6 st dev\n",
    "cleaned_categorical = categorical[not_outliers]\n",
    "cleaned_categorical.reindex()\n",
    "tfm = tfm[not_outliers]\n",
    "tfidf_transformed = tfidf_transformed[not_outliers]\n",
    "\n",
    "print(\"post-filtering \",len(cleaned_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>log(price)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1478823.000</td>\n",
       "      <td>1478823.000</td>\n",
       "      <td>1478823.000</td>\n",
       "      <td>1478823.000</td>\n",
       "      <td>1478823.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>741229.160</td>\n",
       "      <td>1.907</td>\n",
       "      <td>25.753</td>\n",
       "      <td>0.447</td>\n",
       "      <td>2.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>427967.781</td>\n",
       "      <td>0.903</td>\n",
       "      <td>29.028</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>370559.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>741224.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>17.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1111850.500</td>\n",
       "      <td>3.000</td>\n",
       "      <td>29.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1482534.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>347.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id  item_condition_id       price    shipping  log(price)\n",
       "count 1478823.000        1478823.000 1478823.000 1478823.000 1478823.000\n",
       "mean   741229.160              1.907      25.753       0.447       2.907\n",
       "std    427967.781              0.903      29.028       0.497       0.776\n",
       "min         0.000              1.000       3.000       0.000       1.099\n",
       "25%    370559.500              1.000      10.000       0.000       2.303\n",
       "50%    741224.000              2.000      17.000       0.000       2.833\n",
       "75%   1111850.500              3.000      29.000       1.000       3.367\n",
       "max   1482534.000              5.000     347.000       1.000       5.849"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_categorical.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2862: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_categorical[\"brand_name\"][cleaned_categorical[\"brand_name\"].isnull()] = \"No Brand Info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x124203f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD8CAYAAAChHgmuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFV1JREFUeJzt3X+w5XV93/Hny10JPyICYbVkl+Ri\nsiU1TC24AVJaa0FgEeOSTmjJpLJlSLZj0WjTmbg4bUk1zJCZVJSpJSGwcSEqImrYhjXbFTWaGfmx\nC1R+6bCDFG4gsskioEYJ+u4f57N4utwf596933Punvt8zJy53+/nfL7f8/7OMLz28/l+zvekqpAk\nqUsvG3UBkqTxZ9hIkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOrd81AUsFkcf\nfXRNTEyMugxJOqDs3Lnzb6pqxWz9DJtmYmKCHTt2jLoMSTqgJPm/g/RzGk2S1DnDRpLUOcNGktQ5\nw0aS1LnOwibJpiRPJbm/r+2oJNuTPNz+Htnak+SqJLuSfDXJSX3HrG/9H06yvq/99Unua8dclSQz\nfYYkaXS6HNl8BFi7T9tG4LaqWg3c1vYBzgFWt9cG4GroBQdwGXAKcDJwWV94XN367j1u7SyfIUka\nkc7Cpqq+BOzZp3kdsLltbwbO62u/vnpuB45IcgxwNrC9qvZU1dPAdmBte+/wqvpK9X5q9Pp9zjXV\nZ0iSRmTY92xeXVVPArS/r2rtK4HH+/pNtraZ2ienaJ/pM14iyYYkO5Ls2L1797wvSpI0s8WyQCBT\ntNU82uekqq6pqjVVtWbFilm/ACtJmqdhP0Hgm0mOqaon21TYU619Eji2r98q4InW/sZ92r/Y2ldN\n0X+mz9AiMbHx1he3H73i3BFWImlYhj2y2QLsXVG2Hrilr/3CtirtVOCZNgW2DTgryZFtYcBZwLb2\n3nNJTm2r0C7c51xTfYYkaUQ6G9kk+Ti9UcnRSSbprSq7ArgpycXAY8D5rftW4M3ALuC7wEUAVbUn\nyfuBu1q/91XV3kUHb6e34u0Q4LPtxQyfIUkakc7Cpqp+dZq3zpiibwGXTHOeTcCmKdp3ACdM0f63\nU32GJGl0FssCAUnSGDNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJ\nnTNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0z\nbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdG0nY\nJPmPSR5Icn+Sjyc5OMlxSe5I8nCSTyQ5qPX9sba/q70/0XeeS1v715Oc3de+trXtSrJx+FcoSeo3\n9LBJshL4TWBNVZ0ALAMuAH4PuLKqVgNPAxe3Qy4Gnq6qnwWubP1I8tp23M8Da4H/mWRZkmXAh4Fz\ngNcCv9r6SpJGZFTTaMuBQ5IsBw4FngROB25u728Gzmvb69o+7f0zkqS131hV36+qbwC7gJPba1dV\nPVJVzwM3tr6SpBEZethU1V8Bvw88Ri9kngF2At+qqhdat0lgZdteCTzejn2h9f+J/vZ9jpmuXZI0\nIqOYRjuS3kjjOOAngcPoTXntq/YeMs17c22fqpYNSXYk2bF79+7ZSpckzdMoptHeBHyjqnZX1d8D\nnwb+KXBEm1YDWAU80bYngWMB2vuvBPb0t+9zzHTtL1FV11TVmqpas2LFioW4NknSFEYRNo8BpyY5\ntN17OQN4EPgC8Cutz3rglra9pe3T3v98VVVrv6CtVjsOWA3cCdwFrG6r2w6it4hgyxCuSx2Z2Hjr\niy9JB6bls3dZWFV1R5KbgbuBF4B7gGuAW4Ebk/xua7uuHXIdcEOSXfRGNBe08zyQ5CZ6QfUCcElV\n/QAgyTuAbfRWum2qqgeGdX2SpJcaetgAVNVlwGX7ND9CbyXZvn2/B5w/zXkuBy6fon0rsHX/K5Uk\nLQSfICBJ6pxhI0nq3Eim0aSF1r944NErzh1hJZKm4shGktQ5w0aS1DnDRpLUOcNGktQ5w0aS1DnD\nRpLUOcNGktQ5w0aS1DnDRpLUOcNGktQ5w0aS1DnDRpLUOcNGktQ5w0aS1DnDRpLUOcNGktQ5w0aS\n1DnDRpLUOcNGktQ5w0aS1Lnloy5AGqaJjbe+uP3oFeeOsBJpaXFkI0nqnGEjSeqcYSNJ6pxhI0nq\nnGEjSercQKvRkpxQVfd3XYzGiyu/JO016MjmD5LcmeQ/JDmi04okSWNnoLCpqn8G/BpwLLAjyceS\nnNlpZZKksTHwPZuqehj4z8B7gH8BXJXka0n+VVfFSZLGw0Bhk+QfJ7kSeAg4HfilqvpHbfvKuX5o\nkiOS3NzC6qEkv5jkqCTbkzzc/h7Z+ibJVUl2JflqkpP6zrO+9X84yfq+9tcnua8dc1WSzLVGSdLC\nGXRk8z+Au4HXVdUlVXU3QFU9QW+0M1cfAv68qn4OeB29ENsI3FZVq4Hb2j7AOcDq9toAXA2Q5Cjg\nMuAU4GTgsr0B1fps6Dtu7Txq1AwmNt764kuSZjNo2LwZ+FhV/R1AkpclORSgqm6YywcmORx4A3Bd\nO/75qvoWsA7Y3LptBs5r2+uA66vnduCIJMcAZwPbq2pPVT0NbAfWtvcOr6qvVFUB1/edS5I0AoOG\nzeeAQ/r2D21t8/EaYDfwx0nuSXJtksOAV1fVkwDt76ta/5XA433HT7a2mdonp2iXJI3IoGFzcFV9\ne+9O2z50np+5HDgJuLqqTgS+w4+mzKYy1f2Wmkf7S0+cbEiyI8mO3bt3z1y1JGneBg2b7+xzY/71\nwN/N8zMngcmquqPt30wvfL7ZpsBof5/q639s3/GrgCdmaV81RftLVNU1VbWmqtasWLFinpcjSZrN\noGHzbuCTSb6c5MvAJ4B3zOcDq+qvgceTHN+azgAeBLYAe1eUrQduadtbgAvbqrRTgWfaNNs24Kwk\nR7aFAWcB29p7zyU5ta1Cu7DvXJKkERjocTVVdVeSnwOOpzdN9bWq+vv9+Nx3Ah9NchDwCHARveC7\nKcnFwGPA+a3vVnoLFHYB3219qao9Sd4P3NX6va+q9rTttwMfoXef6bPtJUkakbn8UucvABPtmBOT\nUFXXz+dDq+peYM0Ub50xRd8CLpnmPJuATVO07wBOmE9tOvDtuxzb57JJozfogzhvAH4GuBf4QWve\nu6xYkqQZDTqyWQO8to0yJEmak0HD5n7gHwBPdliLtOj4MwnSwhg0bI4GHkxyJ/D9vY1V9dZOqpIk\njZVBw+Z3uixCkjTeBl36/BdJfhpYXVWfa89FW9ZtaZKkcTHoTwz8Br1v+v9ha1oJ/GlXRUmSxsug\nTxC4BDgNeBZe/CG1V814hCRJzaBh8/2qen7vTpLlTPNwS0mS9jVo2PxFkvcChyQ5E/gk8L+6K0uS\nNE4GDZuN9H6D5j7g39N7Xtl8fqFTkrQEDboa7YfAH7WXxpxfZJS00AZ9Nto3mOIeTVW9ZsErkiSN\nnbk8G22vg+k9/v+ohS9HkjSOBrpnU1V/2/f6q6r6IHB6x7VJksbEoNNoJ/XtvozeSOcVnVQkSRo7\ng06j/fe+7ReAR4F/veDVSJLG0qCr0f5l14VIksbXoNNovzXT+1X1gYUpR5I0juayGu0XgC1t/5eA\nLwGPd1GUJGm8zOXH006qqucAkvwO8Mmq+vWuCpMkjY9BH1fzU8DzffvPAxMLXo0kaSwNOrK5Abgz\nyWfoPUngl4HrO6tKkjRWBl2NdnmSzwL/vDVdVFX3dFeWJGmcDDqNBnAo8GxVfQiYTHJcRzVJksbM\noD8LfRnwHuDS1vRy4E+6KkqSNF4GHdn8MvBW4DsAVfUEPq5GkjSgQcPm+aoq2s8MJDmsu5IkSeNm\n0LC5KckfAkck+Q3gc/hDapKkAQ26Gu33k5wJPAscD/zXqtreaWWSpLExa9gkWQZsq6o3AQaMJGnO\nZp1Gq6ofAN9N8soh1CNJGkODPkHge8B9SbbTVqQBVNVvdlKVJGmsDLpA4Fbgv9B70vPOvte8JVmW\n5J4kf9b2j0tyR5KHk3wiyUGt/cfa/q72/kTfOS5t7V9PcnZf+9rWtivJxv2pU5K0/2Yc2ST5qap6\nrKo2d/DZ7wIeAg5v+78HXFlVNyb5A+Bi4Or29+mq+tkkF7R+/ybJa4ELgJ8HfhL4XJJ/2M71YeBM\nYBK4K8mWqnqwg2vQmJjYeOuL249ece4IK5HG02wjmz/du5HkUwv1oUlWAecC17b9AKcDN7cum4Hz\n2va6tk97/4zWfx1wY1V9v6q+AewCTm6vXVX1SFU9D9zY+kqdm9h464svST8yW9ikb/s1C/i5HwR+\nG/hh2/8J4FtV9ULbnwRWtu2VtB9pa+8/0/q/2L7PMdO1v0SSDUl2JNmxe/fu/b0mSdI0ZlsgUNNs\nz1uStwBPVdXOJG/c2zzDZ0/33nTtUwXolLVX1TXANQBr1qxZkOvT/DmVJY2v2cLmdUmepfc/9kPa\nNm2/qurw6Q+d1mnAW5O8GTiY3j2bD9J7OsHyNnpZBTzR+k8Cx9J70vRy4JXAnr72vfqPma5dkjQC\nM06jVdWyqjq8ql5RVcvb9t79+QQNVXVpVa2qqgl6N/g/X1W/BnwB+JXWbT1wS9ve0vZp73++Padt\nC3BBW612HLAauBO4C1jdVrcd1D5jy3xqlSQtjEG/ZzMM7wFuTPK7wD3Ada39OuCGJLvojWguAKiq\nB5LcBDwIvABc0r6ASpJ3ANuAZcCmqnpgqFciSfr/jDRsquqLwBfb9iP0VpLt2+d7wPnTHH85cPkU\n7VuBrQtYqpYQV5JJC28uv9QpSdK8LKZpNGlW47ZibdyuR5qOIxtJUucMG0lS5wwbSVLnvGcjHUC8\nx6MDlSMbSVLnDBtJUuecRpOGwOkvLXWObCRJnXNkI3XEx95IP2LYLDFO50gaBcPmALbvv5wND0mL\nlfdsJEmdM2wkSZ0zbCRJnTNsJEmdc4GANCBX8knz58hGktQ5RzbSPDjKkebGkY0kqXOObKRFyJGT\nxo0jG0lS5wwbSVLnDBtJUue8ZyMtEv4kgcaZIxtJUucMG0lS5wwbSVLnDBtJUucMG0lS51yNJi1R\nPqVAwzT0kU2SY5N8IclDSR5I8q7WflSS7Ukebn+PbO1JclWSXUm+muSkvnOtb/0fTrK+r/31Se5r\nx1yVJMO+TqlrExtvffElLXajGNm8APynqro7ySuAnUm2A/8OuK2qrkiyEdgIvAc4B1jdXqcAVwOn\nJDkKuAxYA1Q7z5aqerr12QDcDmwF1gKfHeI16gDg/6Sl4Rn6yKaqnqyqu9v2c8BDwEpgHbC5ddsM\nnNe21wHXV8/twBFJjgHOBrZX1Z4WMNuBte29w6vqK1VVwPV955IkjcBIFwgkmQBOBO4AXl1VT0Iv\nkIBXtW4rgcf7DptsbTO1T07RLkkakZGFTZIfBz4FvLuqnp2p6xRtNY/2qWrYkGRHkh27d++erWRJ\n0jyNJGySvJxe0Hy0qj7dmr/ZpsBof59q7ZPAsX2HrwKemKV91RTtL1FV11TVmqpas2LFiv27KEnS\ntEaxGi3AdcBDVfWBvre2AHtXlK0Hbulrv7CtSjsVeKZNs20DzkpyZFu5dhawrb33XJJT22dd2Hcu\nSdIIjGI12mnA24D7ktzb2t4LXAHclORi4DHg/PbeVuDNwC7gu8BFAFW1J8n7gbtav/dV1Z62/Xbg\nI8Ah9FahdboSze8rSNLMhh42VfWXTH1fBeCMKfoXcMk059oEbJqifQdwwn6UKR1Q/AePFjsfVyNJ\n6pxhI0nqnM9Gk8aMU2pajAybRcT/SWgx8L9DdcFpNElS5wwbSVLnDBtJUue8ZyNpIN7L0f4wbKT9\ntJh/F8eA0GLhNJokqXOObKQlYjGPwDT+HNlIkjpn2EiSOuc0mqT94iIEDcKRjSSpc4aNJKlzTqNJ\nQ7YYV4Utxpo0XgwbSdNaqBDyvo6cRpMkdc6RjaQF4whG03FkI0nqnGEjSeqcYSNJ6pz3bCR1Yn9W\nsnnvZ/w4spEkdc6RjbTIjdsXLsftejQYw0bSnA0zMJxSGw9Oo0mSOufIRtIBY5BRjiOhxcmwkXRA\nMlQOLE6jSZI658hG0tiabiFD/0ho3z6Okrph2Eg64M11ddxM/b0v1I2xDZska4EPAcuAa6vqihGX\nJOkA43eCFs5Yhk2SZcCHgTOBSeCuJFuq6sHRViZp3AwSSDNN203VZxyNZdgAJwO7quoRgCQ3AusA\nw0bS0A0SSHO9v3SghdO4hs1K4PG+/UnglBHVIknzNl0ILdQU37BCK1U1lA8apiTnA2dX1a+3/bcB\nJ1fVO/fptwHY0HaPB74+y6mPBv5mgcs9kHj9Xr/Xv3RNd/0/XVUrZjt4XEc2k8CxffurgCf27VRV\n1wDXDHrSJDuqas3+l3dg8vq9fq/f65/v8eP6pc67gNVJjktyEHABsGXENUnSkjWWI5uqeiHJO4Bt\n9JY+b6qqB0ZcliQtWWMZNgBVtRXYusCnHXjKbUx5/Uub17+07df1j+UCAUnS4jKu92wkSYuIYTOA\nJJuSPJXk/lHXMmxJjk3yhSQPJXkgybtGXdMwJTk4yZ1J/k+7/v826ppGIcmyJPck+bNR1zIKSR5N\ncl+Se5PsGHU9w5bkiCQ3J/la+3/BL875HE6jzS7JG4BvA9dX1QmjrmeYkhwDHFNVdyd5BbATOG+p\nPPonSYDDqurbSV4O/CXwrqq6fcSlDVWS3wLWAIdX1VtGXc+wJXkUWFNVS/J7Nkk2A1+uqmvbCt9D\nq+pbczmHI5sBVNWXgD2jrmMUqurJqrq7bT8HPETvCQ1LQvV8u+2+vL2W1L/QkqwCzgWuHXUtGr4k\nhwNvAK4DqKrn5xo0YNhoDpJMACcCd4y2kuFqU0j3Ak8B26tqSV0/8EHgt4EfjrqQESrgfyfZ2Z48\nspS8BtgN/HGbSr02yWFzPYlho4Ek+XHgU8C7q+rZUdczTFX1g6r6J/SeRHFykiUzlZrkLcBTVbVz\n1LWM2GlVdRJwDnBJm1pfKpYDJwFXV9WJwHeAjXM9iWGjWbV7FZ8CPlpVnx51PaPSpg6+CKwdcSnD\ndBrw1nbP4kbg9CR/MtqShq+qnmh/nwI+Q+/J8kvFJDDZN6K/mV74zIlhoxm1G+TXAQ9V1QdGXc+w\nJVmR5Ii2fQjwJuBro61qeKrq0qpaVVUT9B779Pmq+rcjLmuokhzWFsfQpo/OApbMytSq+mvg8STH\nt6YzmMfPtYztEwQWUpKPA28Ejk4yCVxWVdeNtqqhOQ14G3Bfu28B8N72hIal4Bhgc/tBvpcBN1XV\nklz+u4S9GvhM799dLAc+VlV/PtqShu6dwEfbSrRHgIvmegKXPkuSOuc0miSpc4aNJKlzho0kqXOG\njSSpc4aNJKlzho0kqXOGjSSpc4aNJKlz/w/fn6isBpjy7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107196d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "cleaned_categorical['log(price)'].plot.hist(bins= 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y_categorical = cleaned_categorical['price'].astype('float', copy=False)\n",
    "cleaned_categorical.drop([\"train_id\",\"name\",\"category_name\",\"item_description\",\"price\", 'log(price)'], axis=1, inplace=True)\n",
    "cleaned_categorical[\"item_condition_id\"] = cleaned_categorical[\"item_condition_id\"].astype('str', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe_cleaned_categorical = pd.get_dummies(cleaned_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Train sample size\n",
    "import numpy as np\n",
    "n_sample = 100000\n",
    "sample = np.random.permutation(cleaned_categorical.shape[0])[:n_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:696: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    }
   ],
   "source": [
    "y_categorical_sample = y_categorical[sample]\n",
    "ohe_cleaned_categorical_sample = ohe_cleaned_categorical.iloc[sample]\n",
    "tfidf_transformed_sample = tfidf_transformed[sample]\n",
    "\n",
    "ohe_cleaned_categorical_sample.reindex();\n",
    "y_categorical_sample = np.asarray(y_categorical_sample, dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "yTrain, yTest, XTrain, XTest, tfidfTrain, tfidfTest = train_test_split(\n",
    "    y_categorical_sample, ohe_cleaned_categorical_sample, tfidf_transformed_sample, test_size=0.3, random_state=95)\n",
    "\n",
    "XTrain.reindex()\n",
    "XTest.reindex();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of folds\n",
    "from sklearn.model_selection import KFold\n",
    "folds = 2\n",
    "kf = KFold(n_splits=folds)\n",
    "kf_inner = KFold(n_splits=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 8 µs, total: 13 µs\n",
      "Wall time: 31.9 µs\n",
      "0\n",
      "0 0.01\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Test train and validate\n",
    "alpha_values = [0.01,0.1, 1, 10]\n",
    "from sklearn.linear_model import Lasso\n",
    "import copy\n",
    "\n",
    "mseOuterTrain = []\n",
    "mseOuterVal = []\n",
    "mseInnerTrain = []\n",
    "mseInnerVal = []\n",
    "mseOverallTrain = []\n",
    "mseOverallVal= []\n",
    "\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(yTrain)):\n",
    "    print(i)\n",
    "    yTrain_fold = yTrain[train_index]\n",
    "    yVal_fold = yTrain[val_index]\n",
    "\n",
    "    XTrain_fold = XTrain.iloc[train_index]\n",
    "    XVal_fold = XTrain.iloc[val_index]\n",
    "    \n",
    "\n",
    "    for a_outer in alpha_values:\n",
    "        print(i, a_outer)\n",
    "        lasso = Lasso(alpha=a_outer, random_state=111, max_iter=1000).fit(XTrain_fold, yTrain_fold)\n",
    "        predict_train = lasso.predict(XTrain_fold)\n",
    "        train_residuals = yTrain_fold - predict_train\n",
    "        mseOuterTrain.append(copy.deepcopy((i, None, a_outer ,None, np.square(train_residuals).mean())))\n",
    "        print((i, None, a_outer , None, np.square(train_residuals).mean()))\n",
    "        \n",
    "        predict_val = lasso.predict(XVal_fold)\n",
    "        val_residuals = yVal_fold - predict_val\n",
    "        mseOuterVal.append(copy.deepcopy((i, None, a_outer , None, np.square(val_residuals).mean())))\n",
    "        print((i, None, a_outer , None, np.square(val_residuals).mean()))\n",
    "        \n",
    "        outer_residuals = yTrain - lasso.predict(XTrain)\n",
    "            \n",
    "        for j, (train_inner_index, val_inner_index) in enumerate(kf_inner.split(tfidfTrain)):\n",
    "            tfidfTrain_fold = tfidfTrain[train_inner_index]\n",
    "            tfidfVal_fold = tfidfTrain[val_inner_index]\n",
    "            \n",
    "            outer_residuals_train = outer_residuals[train_inner_index]\n",
    "            outer_residuals_val = outer_residuals[val_inner_index]\n",
    "        \n",
    "            for a_inner in alpha_values:\n",
    "                print(j, a_outer, a_inner)\n",
    "                lasso_inner = Lasso(alpha=a_inner, random_state=111, max_iter=1000).fit(tfidfTrain_fold, outer_residuals_train)\n",
    "                predict_inner_train = lasso_inner.predict(tfidfTrain_fold)\n",
    "                inner_train_residuals = train_residuals - predict_inner_train\n",
    "                mseInnerTrain.append(copy.deepcopy((i, j, a_inner, np.square(inner_train_residuals).mean())))\n",
    "\n",
    "                predict_inner_val = lasso_inner.predict(tfidfVal_fold)\n",
    "                inner_val_residuals = val_residuals - predict_inner_val\n",
    "                mseInnerVal.append(copy.deepcopy((i, j, a_inner, np.square(inner_val_residuals).mean())))\n",
    "                print((i, j, a_inner, np.square(inner_val_residuals).mean()))\n",
    "            \n",
    "                residuals_overall_train = yTrain_fold - predict_train + predict_inner_train\n",
    "                mseOverallTrain.append(copy.deepcopy((i, j, a_outer, a_inner, np.square(residuals_overall_train).mean())))\n",
    "            \n",
    "                residuals_overall_val = yVal_fold - predict_val + predict_inner_val\n",
    "                mseOverallVal.append(copy.deepcopy((i, j, a_outer, a_inner, np.square(residuals_overall_val).mean())))\n",
    "                print((i, j, a_outer, a_inner, np.square(residuals_overall_val).mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted(mseOverallVal, key = operator.itemgetter(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Category (non-text) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('Data/vocabulary.txt', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "#alpha_values = np.logspace(-4, 2, 6)\n",
    "alpha_values = [0.001,0.01,0.1]\n",
    "\n",
    "mseRow = []\n",
    "mseAll = []\n",
    "mseOuterTrain = []\n",
    "mseOuterValidate = []\n",
    "mseInner = []\n",
    "\n",
    "#manually grid search over alpha\n",
    "for alphaOut in alpha_values:\n",
    "    \n",
    "    #non-Text data\n",
    "    lasso = Lasso(alpha=alphaOut, random_state=111, max_iter=1000).fit(XTrain, yTrain)\n",
    "    yPredOut = lasso.predict(XValid)\n",
    "    residuals = yValid - yPredOut\n",
    "    \n",
    "    mseRow = []\n",
    "    \n",
    "    #can't figure out how to implement a validation or test accuracy as the training of the inner model is dependent \n",
    "    # on the outermodel's predictions.  If we predict on XValid, then we need to train the inner model on XValid.\n",
    "    # not sure if this makes sense\n",
    "    \n",
    "    #GridSearch manually\n",
    "    for alphaIn in alpha_values:\n",
    "        lassoTF = Lasso(alpha=alphaIn, random_state=777, max_iter=1000).fit(tfidfValid, residuals)\n",
    "        yPredIn = lassoTF.predict(tfidfValid)\n",
    "        \n",
    "        #add predictions from non-Text and Text regressions\n",
    "        yPredFull = yPredOut + yPredIn\n",
    "        \n",
    "        #calc mse between full predictions & actuals\n",
    "        mseRow.append(mean_squared_error(yValid, yPredFull))\n",
    "    \n",
    "    #store for of mse's\n",
    "    mseAll.append(mseRow)\n",
    "    \n",
    "mseAll = np.array(mseAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get bestAlphaOut & In based on the smallest mse from gridsearch\n",
    "bestAlpha_idx = np.unravel_index(np.argmin(mseAll), mseAll.shape)\n",
    "#bestAlpha_idx[0]\n",
    "bestAlphaOut = alpha_values[bestAlpha_idx[0]]\n",
    "bestAlphaIn = alpha_values[bestAlpha_idx[1]]\n",
    "print()\n",
    "print(\"Best Alpha for Outer Lasso (non-text): {}\".format(bestAlphaOut))\n",
    "print(\"Best Alpha for Linner Lasso (text): {}\".format(bestAlphaIn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run final model with bestAlphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=bestAlphaOut, random_state=111).fit(XTrain, yTrain)\n",
    "yPredOut = lasso.predict(XValid)\n",
    "residualsBest = yValid - yPredOut\n",
    "\n",
    "lassoTF = Lasso(alpha=bestAlphaIn, random_state=111).fit(tfidfTrain, residualsBest)\n",
    "yPredIn = lasso.predict(tfidfValid)\n",
    "\n",
    "yPredFull = yPredOut + yPredIn\n",
    "\n",
    "mse_Final = mean_squared_error(yValid, yPredFull)\n",
    "print(\"The MSE for the final tuned models is: {}\".format(mse_Final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(lasso, 'lasso_cat.pkl')\n",
    "joblib.dump(lassTF, 'lasso_tfidf.pkl')\n",
    "#clf = joblib.load('lasso_cat.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine non-Text & text data and run Lasso to see if prediction is diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine non-Text and Text to test it out\n",
    "# tfDTrain = tfidfTrain.todense()\n",
    "# comboTrain = pd.concat([XTrain.reset_index(), pd.DataFrame(tfDTrain)], axis=1)\n",
    "# comboTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save lasso on category data as DataFrame, write to csv\n",
    "# lasso_cat_coef = pd.DataFrame(lasso.coef_, index=XTrain.columns)\n",
    "# lasso_cat_coef.to_csv(\"lasso_cat_coef.csv\")\n",
    "\n",
    "# residualsDF = pd.DataFrame(residuals)\n",
    "# residualsDF.to_csv(\"residuals.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Text data on residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save lasso on TFIDF data as DataFrame, write to csv\n",
    "# lasso_tfidf_coef = pd.DataFrame(lassoTF.coef_, index=vocab.iloc[:,0].values)\n",
    "# lasso_tfidf_coef.to_csv(\"lasso_tfidf_coef.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Text data on residuals (per category1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the category1 column headers\n",
    "cat1_cols = XTrain.iloc[:,4815:4825].columns\n",
    "\n",
    "#reset index so can filter using list of indices\n",
    "residuals.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cat1_coefs = []\n",
    "tfidfTrain_df = pd.DataFrame(tfidfTrain.todense())  #I think can remove this and uncomment tocsr() line below\n",
    "\n",
    "for cat1 in cat1_cols:\n",
    "    #get indices where this category=1\n",
    "    idx = np.where(XTrain[cat1] == 1)[0]\n",
    "   \n",
    "    #filter data with the indices\n",
    "    X_tf = tfidfTrain_df.iloc[idx,:]   #Don't think need to use this, uncomment next time to deal with sparse\n",
    "    #X_tf = tfidfTrain.tocsr()[idx,:]  #way to filter sparse matrx with row indices\n",
    "    y_tf = residuals[idx]\n",
    "    \n",
    "    #run Lasso, fit, store coef array\n",
    "    lassoCat = Lasso(alpha=bestAlphaIn, random_state=10)\n",
    "    lassoCat.fit(X_tf, y_tf)\n",
    "\n",
    "    #add this cat1 coefs to a list\n",
    "    cat1_coefs.append(lassoCat.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#covert to DF with cols=vocab and index=category1 names\n",
    "lasso_allCat1_coef = pd.DataFrame(cat1_coefs, columns=vocab.iloc[:,0].values, index=cat1_cols)\n",
    "lasso_allCat1_coef.to_csv(\"lass_allCat1_coef.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><br>\n",
    "\n",
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
    "# from keras.layers.recurrent import LSTM, RNN, GRU, SimpleRNN\n",
    "# from keras.models import load_model\n",
    "# import keras\n",
    "# import h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #default layers = 6, neurons list always should equal 6\n",
    "# def build_NN(inShape, nnType='RNN'):\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(inShape[1], 64, input_length=maxLen))  #input layer\n",
    "#     if nnType == 'RNN':\n",
    "#         model.add(SimpleRNN(32, return_sequences=True))\n",
    "#         model.add(SimpleRNN(32))\n",
    "#         model.add(Dense(16, activation='relu'))\n",
    "#         model.add(Dense(8, activation='relu'))\n",
    "#     elif nnType == 'GRU':\n",
    "#         #model.add(Dense(64, input_shape=(inShape[1])))  #input layer\n",
    "#         model.add(GRU(32, return_sequences=True))\n",
    "#         model.add(GRU(32))\n",
    "#         model.add(Dense(16, activation='relu'))\n",
    "#         model.add(Dense(8, activation='relu'))\n",
    "#     elif nnType == 'LSTM':\n",
    "#         #model.add(Dense(64, input_shape=(inShape[1])))  #input layer\n",
    "#         model.add(LSTM(32, return_sequences=True)) \n",
    "#         model.add(LSTM(32))\n",
    "#         model.add(Dense(16, activation='relu'))      \n",
    "#         model.add(Dense(8, activation='relu'))\n",
    "#     elif nnType == 'BASE':\n",
    "#         #model.add(Dense(64, input_shape=(inShape[1],)))  #input layer\n",
    "#         model.add(Dense(32, activation='relu'))\n",
    "#         model.add(Dense(32, activation='relu'))\n",
    "#         model.add(Dense(16, activation='relu'))\n",
    "#         model.add(Reshape((-1,)))\n",
    "#         model.add(Dense(8, activation='relu'))\n",
    "#     else:\n",
    "#         print('should not be here')\n",
    "    \n",
    "#     model.add(Dense(1, activation='linear', kernel_initializer=\"uniform\")) #output layer\n",
    "#     model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "#     #model.summary() \n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #params for NN\n",
    "# epochs = 10\n",
    "# batchSize = 3000\n",
    "\n",
    "# #Need to prep the data for the NN\n",
    "\n",
    "# X = tokenizer.texts_to_sequences(train5k)\n",
    "# data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "\n",
    "# modelRNN = build_NN(tfidf_train.shape, nnType='RNN')\n",
    "# modelRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
