{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPS - Mercari Price ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data/train.tsv\", delimiter=\"\\t\", index_col=0)\n",
    "\n",
    "# remove items with out a price\n",
    "data = data[pd.notna(data[\"price\"])]\n",
    "\n",
    "data[\"item_description\"] = data[\"item_description\"].replace(\"No description yet\", \"\")\n",
    "data[\"item_description\"] = data[\"item_description\"].replace(np.nan, \"\")\n",
    "\n",
    "temp = data[\"category_name\"].fillna('').str.split('/')\n",
    "              \n",
    "data[\"category_name_1\"] = temp.str[0]\n",
    "data[\"category_name_2\"] = temp.str[1]\n",
    "data[\"category_name_3\"] = temp.str[2:].str.join(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement porter stemming in count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "\n",
    "class StemmerTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.ps = PorterStemmer()\n",
    "        self.translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "    def __call__(self, doc):\n",
    "        return [self.ps.stem(w) for w in doc\n",
    "                .encode('ascii', errors='ignore')\n",
    "                .decode('ascii')\n",
    "                .translate(self.translator)\n",
    "                .split()]\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase = True,\n",
    "                             max_df = .5,\n",
    "                             min_df = .001,\n",
    "                             tokenizer = StemmerTokenizer(),\n",
    "                             stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tfm = vectorizer.fit_transform(data[\"item_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_ \n",
    "removed_words = vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "tfidf_transformed = tfidf_vectorizer.fit_transform(tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results for time savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz\n",
    "save_npz(\"tfm.npz\", tfm)\n",
    "save_npz(\"tfidf_transformed.npz\", tfidf_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab: write vectorized words to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "with open(\"vocabulary.txt\", \"w\") as f:\n",
    "    f.write(\"\".join([k + '\\n' for k, v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><hr><hr>\n",
    "\n",
    "# Load files as necessary for time savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "tfm = load_npz(\"Data/tfm.npz\")\n",
    "tfidf_transformed = load_npz(\"Data/tfidf_transformed.npz\")\n",
    "categorical = pd.read_csv('Data/train_clean.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-filtering  1482535\n",
      "post-filtering  1442266\n"
     ]
    }
   ],
   "source": [
    "# take the log of price (0.000000001 added for item with 0 as price)\n",
    "categorical['log(price)'] = np.log(categorical['price']+ 0.000000001)\n",
    "\n",
    "# calculate 6 st dev of log(price)\n",
    "st_dev_log_price_6 = categorical['log(price)'].std()*6\n",
    "\n",
    "print(\"pre-filtering \",len(categorical))\n",
    "\n",
    "# find points that are not outliers\n",
    "not_outliers = np.array((categorical['log(price)'] < st_dev_log_price_3) & (categorical['log(price)'] > (- st_dev_log_price_3)))\n",
    "\n",
    "# filter dataset to data only around 6 st dev\n",
    "cleaned_categorical = categorical[not_outliers]\n",
    "cleaned_categorical.reindex()\n",
    "tfm = tfm[not_outliers]\n",
    "tfidf_transformed = tfidf_transformed[not_outliers]\n",
    "\n",
    "print(\"post-filtering \",len(cleaned_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>log(price)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1442266.000</td>\n",
       "      <td>1442266.000</td>\n",
       "      <td>1442266.000</td>\n",
       "      <td>1442266.000</td>\n",
       "      <td>1442266.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>741280.055</td>\n",
       "      <td>1.907</td>\n",
       "      <td>22.291</td>\n",
       "      <td>0.451</td>\n",
       "      <td>2.852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>427996.416</td>\n",
       "      <td>0.903</td>\n",
       "      <td>17.456</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>370587.250</td>\n",
       "      <td>1.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>741286.500</td>\n",
       "      <td>2.000</td>\n",
       "      <td>16.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1111962.750</td>\n",
       "      <td>3.000</td>\n",
       "      <td>28.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1482534.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>105.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id  item_condition_id       price    shipping  log(price)\n",
       "count 1442266.000        1442266.000 1442266.000 1442266.000 1442266.000\n",
       "mean   741280.055              1.907      22.291       0.451       2.852\n",
       "std    427996.416              0.903      17.456       0.498       0.705\n",
       "min         0.000              1.000       3.000       0.000       1.099\n",
       "25%    370587.250              1.000      10.000       0.000       2.303\n",
       "50%    741286.500              2.000      16.000       0.000       2.773\n",
       "75%   1111962.750              3.000      28.000       1.000       3.332\n",
       "max   1482534.000              5.000     105.000       1.000       4.654"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_categorical.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2862: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_categorical[\"brand_name\"][cleaned_categorical[\"brand_name\"].isnull()] = \"No Brand Info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f278fa90>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD8CAYAAAChHgmuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFmtJREFUeJzt3X+w5XV93/Hny10QUBGQ1ZBdzMVm\na6pMrLBBUtvUisAKCUtaaTeTyspgtmMwatOZuDhtSDXMkJnUHzQWQ2TrQlRAtLIRKLOCxnRGfixg\n5ZeWHaSwgcrGRSD+gKy++8f5XHK43Lv33Mv93nPu2edj5sz9fj/fz/ec9/nu7H3dz/f7Od+TqkKS\npC69YNgFSJLGn2EjSeqcYSNJ6pxhI0nqnGEjSeqcYSNJ6pxhI0nqnGEjSeqcYSNJ6tzyYRcwKg4/\n/PCamJgYdhmStKTcdtttf1NVK2brZ9g0ExMTbN++fdhlSNKSkuT/DtLP02iSpM4ZNpKkzhk2kqTO\nGTaSpM51FjZJNid5NMldfW2HJdmW5L7289DWniQXJtmR5JtJjunbZ0Prf1+SDX3txya5s+1zYZLs\n7TUkScPT5cjmU8DaKW2bgBuqajVwQ1sHeCuwuj02AhdBLziA84A3AMcB5/WFx0Wt7+R+a2d5DUnS\nkHQWNlX1NWD3lOZ1wJa2vAU4va/90uq5CTgkyRHAycC2qtpdVY8B24C1bdvBVfX16n3V6KVTnmu6\n15AkDcliX7N5RVU9AtB+vry1rwQe6uu3s7XtrX3nNO17e43nSLIxyfYk23ft2jXvNyVJ2rtRmSCQ\nadpqHu1zUlUXV9WaqlqzYsWsH4CVJM3TYt9B4LtJjqiqR9qpsEdb+07gyL5+q4CHW/ubprR/tbWv\nmqb/3l5D+4iJTdc8a/2BC04dUiWSJi32yGYrMDmjbANwdV/7mW1W2vHA4+0U2PXASUkObRMDTgKu\nb9ueTHJ8m4V25pTnmu41JElD0tnIJsln6Y1KDk+yk96ssguAK5OcDTwInNG6XwucAuwAfgicBVBV\nu5N8CLi19ftgVU1OOngXvRlvBwLXtQd7eQ1J0pB0FjZV9RszbDphmr4FnDPD82wGNk/Tvh04epr2\n7033GpKk4RmVCQKSpDFm2EiSOmfYSJI6Z9hIkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOmfYSJI6Z9hI\nkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOmfYSJI6\nZ9hIkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOmfYSJI6Z9hIkjpn2EiSOjeU\nsEny75PcneSuJJ9NckCSo5LcnOS+JFck2b/1fWFb39G2T/Q9z7mt/dtJTu5rX9vadiTZtPjvUJLU\nb9HDJslK4D3Amqo6GlgGrAf+CPhIVa0GHgPObrucDTxWVT8PfKT1I8lr2n6vBdYC/y3JsiTLgI8D\nbwVeA/xG6ytJGpJhnUZbDhyYZDlwEPAI8GbgqrZ9C3B6W17X1mnbT0iS1n55VT1VVd8BdgDHtceO\nqrq/qp4GLm99JUlDsuhhU1V/Dfwx8CC9kHkcuA34flXtad12Aivb8krgobbvntb/Zf3tU/aZqV2S\nNCTDOI12KL2RxlHAzwIvonfKa6qa3GWGbXNtn66WjUm2J9m+a9eu2UqXJM3TME6jvQX4TlXtqqq/\nA74A/BPgkHZaDWAV8HBb3gkcCdC2vxTY3d8+ZZ+Z2p+jqi6uqjVVtWbFihUL8d4kSdMYRtg8CByf\n5KB27eUE4B7gK8DbWp8NwNVteWtbp22/saqqta9vs9WOAlYDtwC3Aqvb7Lb96U0i2LoI70tjYmLT\nNc88JC2M5bN3WVhVdXOSq4DbgT3AHcDFwDXA5Un+sLVd0na5BLgsyQ56I5r17XnuTnIlvaDaA5xT\nVT8BSPJu4Hp6M902V9Xdi/X+JEnPtehhA1BV5wHnTWm+n95Msql9fwycMcPznA+cP037tcC1z79S\nSdJC8A4CkqTOGTaSpM4N5TSaNK76JxU8cMGpQ6xEGi2ObCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmd\nM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNs\nJEmdM2wkSZ0zbCRJnTNsJEmdWz7sAqR92cSma55ZfuCCU4dYidQtRzaSpM4ZNpKkzhk2kqTOGTaS\npM4ZNpKkzg00Gy3J0VV1V9fFSLNx9pa0NA06svlEkluS/HaSQzqtSJI0dgYKm6r6p8BvAkcC25N8\nJsmJnVYmSRobA1+zqar7gP8IvB/458CFSb6V5F92VZwkaTwMFDZJfjHJR4B7gTcDv1ZV/6gtf2Su\nL5rkkCRXtbC6N8kvJzksybYk97Wfh7a+SXJhkh1JvpnkmL7n2dD635dkQ1/7sUnubPtcmCRzrVGS\ntHAGHdn8CXA78LqqOqeqbgeoqofpjXbm6mPA/6yqXwBeRy/ENgE3VNVq4Ia2DvBWYHV7bAQuAkhy\nGHAe8AbgOOC8yYBqfTb27bd2HjVqkU1suuaZh6TxMmjYnAJ8pqp+BJDkBUkOAqiqy+bygkkOBn4F\nuKTt/3RVfR9YB2xp3bYAp7fldcCl1XMTcEiSI4CTgW1VtbuqHgO2AWvbtoOr6utVVcClfc8lSRqC\nQcPmy8CBfesHtbb5eBWwC/jvSe5I8skkLwJeUVWPALSfL2/9VwIP9e2/s7XtrX3nNO2SpCEZNGwO\nqKq/nVxpywfN8zWXA8cAF1XV64Ef8PenzKYz3fWWmkf7c5842Zhke5Ltu3bt2nvVkqR5GzRsfjDl\nwvyxwI/m+Zo7gZ1VdXNbv4pe+Hy3nQKj/Xy0r/+RffuvAh6epX3VNO3PUVUXV9WaqlqzYsWKeb4d\nSdJsBg2b9wGfS/JXSf4KuAJ493xesKr+H/BQkle3phOAe4CtwOSMsg3A1W15K3Bmm5V2PPB4O812\nPXBSkkPbxICTgOvbtieTHN9moZ3Z91ySpCEY6HY1VXVrkl8AXk3vNNW3qurvnsfr/g7w6ST7A/cD\nZ9ELviuTnA08CJzR+l5Lb4LCDuCHrS9VtTvJh4BbW78PVtXutvwu4FP0rjNd1x6SpCGZyzd1/hIw\n0fZ5fRKq6tL5vGhVfQNYM82mE6bpW8A5MzzPZmDzNO3bgaPnU5vUBe/ppn3doDfivAz4B8A3gJ+0\n5slpxZIk7dWgI5s1wGvaKEOSpDkZNGzuAn4GeKTDWiTNk6fpNOoGDZvDgXuS3AI8NdlYVad1UpUk\naawMGjZ/0GURkqTxNujU579M8nPA6qr6crsv2rJuS5MkjYtBv2Lgt+h90v9PW9NK4ItdFSVJGi+D\n3kHgHOCNwBPwzBepvXyve0iS1AwaNk9V1dOTK0mWM8PNLSVJmmrQsPnLJB8ADkxyIvA54C+6K0uS\nNE4GDZtN9L6D5k7g39G7X9l8vqFTkrQPGnQ22k+BP2sPaSB+0FDSpEHvjfYdprlGU1WvWvCKJElj\nZy73Rpt0AL3b/x+28OVIksbRQNdsqup7fY+/rqqPAm/uuDZJ0pgY9DTaMX2rL6A30nlJJxVJksbO\noKfR/kvf8h7gAeBfL3g1kqSxNOhstH/RdSGSpPE16Gm0393b9qr68MKUI0kaR3OZjfZLwNa2/mvA\n14CHuihKkjRe5vLlacdU1ZMASf4A+FxVvbOrwiRJ42PQ29W8Eni6b/1pYGLBq5EkjaVBRzaXAbck\n+R/07iTw68ClnVUlSRorg85GOz/JdcA/a01nVdUd3ZUlSRong55GAzgIeKKqPgbsTHJURzVJksbM\noF8LfR7wfuDc1rQf8OddFSVJGi+Djmx+HTgN+AFAVT2Mt6uRJA1o0LB5uqqK9jUDSV7UXUmSpHEz\naNhcmeRPgUOS/BbwZfwiNUnSgAadjfbHSU4EngBeDfx+VW3rtDJJ0tiYNWySLAOur6q3AAaMJGnO\nZj2NVlU/AX6Y5KWLUI8kaQwNegeBHwN3JtlGm5EGUFXv6aQqSdJYGXSCwDXAf6J3p+fb+h7zlmRZ\nkjuSfKmtH5Xk5iT3Jbkiyf6t/YVtfUfbPtH3HOe29m8nObmvfW1r25Fk0/OpU5L0/O11ZJPklVX1\nYFVt6eC13wvcCxzc1v8I+EhVXZ7kE8DZwEXt52NV9fNJ1rd+/ybJa4D1wGuBnwW+nOQftuf6OHAi\nsBO4NcnWqrqng/cgdW5i0zXPLD9wwalDrESav9lGNl+cXEjy+YV60SSrgFOBT7b1AG8GrmpdtgCn\nt+V1bZ22/YTWfx1weVU9VVXfAXYAx7XHjqq6v6qeBi5vfSXNYGLTNc88pC7MFjbpW37VAr7uR4Hf\nA37a1l8GfL+q9rT1ncDKtryS9iVtbfvjrf8z7VP2man9OZJsTLI9yfZdu3Y93/ckSZrBbBMEaobl\neUvyq8CjVXVbkjdNNu/ltWfaNlP7dAE6be1VdTFwMcCaNWsW5P1p6fD0lLR4Zgub1yV5gt4v9gPb\nMm29qurgmXed0RuB05KcAhxA75rNR+ndnWB5G72sAh5u/XcCR9K70/Ry4KXA7r72Sf37zNQuSRqC\nvZ5Gq6plVXVwVb2kqpa35cn1+QQNVXVuVa2qqgl6F/hvrKrfBL4CvK112wBc3Za3tnXa9hvbfdq2\nAuvbbLWjgNXALcCtwOo2u23/9hpb51OrJGlhDPo5m8XwfuDyJH8I3AFc0tovAS5LsoPeiGY9QFXd\nneRK4B5gD3BO+wAqSd4NXA8sAzZX1d2L+k4kSc8y1LCpqq8CX23L99ObSTa1z4+BM2bY/3zg/Gna\nrwWuXcBSpZHgdSYtVXP5pk5JkuZllE6jSUuGIwxpbhzZSJI6Z9hIkjpn2EiSOmfYSJI65wQBSSPB\nSRfjzZGNJKlzjmwkzcjRhhaKIxtJUucc2Uh6Fr9ATV0wbDQwT6lImi/DZh9jYEgaBq/ZSJI6Z9hI\nkjpn2EiSOmfYSJI65wQBacw4CUSjyJGNJKlzjmykfcTUD2s66tFiMmwkLRmeIly6PI0mSeqcYSNJ\n6pxhI0nqnGEjSeqcEwQkLRgv4GsmjmwkSZ0zbCRJnTNsJEmdM2wkSZ0zbCRJnTNsJEmdW/SwSXJk\nkq8kuTfJ3Une29oPS7ItyX3t56GtPUkuTLIjyTeTHNP3XBta//uSbOhrPzbJnW2fC5Nksd+ntK+Y\n2HTNMw9pJsP4nM0e4D9U1e1JXgLclmQb8A7ghqq6IMkmYBPwfuCtwOr2eANwEfCGJIcB5wFrgGrP\ns7WqHmt9NgI3AdcCa4HrFvE9SnPmZ1QWnsd0dCz6yKaqHqmq29vyk8C9wEpgHbClddsCnN6W1wGX\nVs9NwCFJjgBOBrZV1e4WMNuAtW3bwVX19aoq4NK+55IkDcFQr9kkmQBeD9wMvKKqHoFeIAEvb91W\nAg/17bazte2tfec07ZKkIRla2CR5MfB54H1V9cTeuk7TVvNon66GjUm2J9m+a9eu2UqWJM3TUMIm\nyX70gubTVfWF1vzddgqM9vPR1r4TOLJv91XAw7O0r5qm/Tmq6uKqWlNVa1asWPH83pQkaUbDmI0W\n4BLg3qr6cN+mrcDkjLINwNV97We2WWnHA4+302zXAyclObTNXDsJuL5tezLJ8e21zux7LknSEAxj\nNtobgbcDdyb5Rmv7AHABcGWSs4EHgTPatmuBU4AdwA+BswCqaneSDwG3tn4frKrdbfldwKeAA+nN\nQut0JpozXqT5cbr0vmPRw6aq/hfTX1cBOGGa/gWcM8NzbQY2T9O+HTj6eZQpSVpA3kFAktQ5vzxN\nUic8vax+jmwkSZ1zZLME+Bei1J2pkxT8P9YNRzaSpM4ZNpKkznkaTdKc+fkYzZUjG0lS5xzZSPso\nRydaTI5sJEmdc2QjSR3wIwvP5shGktQ5RzaSNANHJwvHkY0kqXOGjSSpc4aNJKlzXrORNJCl8rmc\nxa7T6zqDMWwkda6LX8j+kl9aDBtJGsBChdu+GpKGjSSNgHEPIScISJI6Z9hIkjrnaTRJS95cZ6At\nlZl148SwkTTSDIbx4Gk0SVLnHNlIWlSOVPZNho0kzdGoBeZSmDZt2EjSCJspSEYt8GZj2EjaJyy1\nX84LbdijHycISJI658hG0sjZ10ch48iwkaQ+oxB0C1XDKLyXSYaNJC0RoxQeczW2YZNkLfAxYBnw\nyaq6YMglSRpzSyUMhjFZYCzDJsky4OPAicBO4NYkW6vqnuFWJmlfNFMIdRFOoxp44zob7ThgR1Xd\nX1VPA5cD64ZckyTts8Y1bFYCD/Wt72xtkqQhSFUNu4YFl+QM4OSqemdbfztwXFX9zpR+G4GNbfXV\nwLdneMrDgb/pqNyFtFTqBGvtirUuvKVSJwyn1p+rqhWzdRrLazb0RjJH9q2vAh6e2qmqLgYunu3J\nkmyvqjULV143lkqdYK1dsdaFt1TqhNGudVxPo90KrE5yVJL9gfXA1iHXJEn7rLEc2VTVniTvBq6n\nN/V5c1XdPeSyJGmfNZZhA1BV1wLXLtDTzXqqbUQslTrBWrtirQtvqdQJI1zrWE4QkCSNlnG9ZiNJ\nGiGGTZNkc5JHk9w1w/YkuTDJjiTfTHLMYtfYV8tstb4pyeNJvtEev7/YNbY6jkzylST3Jrk7yXun\n6TMSx3XAWkfluB6Q5JYk/7vV+p+n6fPCJFe043pzkokRrfMdSXb1HdN3LnadU+pZluSOJF+aZtvQ\nj+mUevZW60gdVwCqykfvVOKvAMcAd82w/RTgOiDA8cDNI1zrm4AvjcAxPQI4pi2/BPg/wGtG8bgO\nWOuoHNcAL27L+wE3A8dP6fPbwCfa8nrgihGt8x3Anwz7mPbV87vAZ6b7dx6FYzqHWkfquFaVI5tJ\nVfU1YPdeuqwDLq2em4BDkhyxONU92wC1joSqeqSqbm/LTwL38tw7OYzEcR2w1pHQjtXfttX92mPq\nxdd1wJa2fBVwQpIsUonAwHWOjCSrgFOBT87QZejHdNIAtY4cw2ZwS+0WOL/cTl9cl+S1wy6mnXJ4\nPb2/bvuN3HHdS60wIse1nUL5BvAosK2qZjyuVbUHeBx42eJWOVCdAP+qnUK9KsmR02xfLB8Ffg/4\n6QzbR+KYNrPVCqNzXAHDZi6m+wtmVP9Ku53eLSReB/xX4IvDLCbJi4HPA++rqiembp5ml6Ed11lq\nHZnjWlU/qap/TO/uGMclOXpKl5E4rgPU+RfARFX9IvBl/n7ksKiS/CrwaFXdtrdu07Qt+jEdsNaR\nOK79DJvBDXQLnFFQVU9Mnr6o3ueN9kty+DBqSbIfvV/en66qL0zTZWSO62y1jtJx7avp+8BXgbVT\nNj1zXJMsB17KEE+9zlRnVX2vqp5qq38GHLvIpU16I3Bakgfo3SX+zUn+fEqfUTmms9Y6Qsf1GYbN\n4LYCZ7bZU8cDj1fVI8MuajpJfmbyXHKS4+j9O39vCHUEuAS4t6o+PEO3kTiug9Q6Qsd1RZJD2vKB\nwFuAb03pthXY0JbfBtxY7crxYhmkzinX506jd61s0VXVuVW1qqom6F38v7Gq/u2UbkM/pjBYraNy\nXPuN7R0E5irJZ+nNNjo8yU7gPHoXNKmqT9C7G8EpwA7gh8BZw6l0oFrfBrwryR7gR8D6YfynoPcX\n2NuBO9t5e4APAK/sq3VUjusgtY7KcT0C2JLelwS+ALiyqr6U5IPA9qraSi84L0uyg95f3+tHtM73\nJDkN2NPqfMcQ6pzRCB7TGY36cfUOApKkznkaTZLUOcNGktQ5w0aS1DnDRpLUOcNGktQ5w0aS1DnD\nRpLUOcNGktS5/w/4tsyrcPOxtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2138dc320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "cleaned_categorical['log(price)'].plot.hist(bins= 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1442266, 1793)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1442266, 1793)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1442266, 12)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y_categorical = cleaned_categorical['price'].astype('float', copy=False)\n",
    "cleaned_categorical.drop([\"train_id\",\"name\",\"category_name\",\"item_description\",\"price\", 'log(price)'], axis=1, inplace=True)\n",
    "cleaned_categorical[\"item_condition_id\"] = cleaned_categorical[\"item_condition_id\"].astype('str', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.455861846373143"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_categorical.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe_cleaned_categorical = pd.get_dummies(cleaned_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Train sample size\n",
    "import numpy as np\n",
    "n_sample = 100000\n",
    "sample = np.random.permutation(cleaned_categorical.shape[0])[:n_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:696: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    }
   ],
   "source": [
    "y_categorical_sample = y_categorical[sample]\n",
    "ohe_cleaned_categorical_sample = ohe_cleaned_categorical.iloc[sample]\n",
    "tfidf_transformed_sample = tfidf_transformed[sample]\n",
    "\n",
    "ohe_cleaned_categorical_sample.reindex();\n",
    "y_categorical_sample = np.asarray(y_categorical_sample, dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "yTrain, yTest, XTrain, XTest, tfidfTrain, tfidfTest = train_test_split(\n",
    "    y_categorical_sample, ohe_cleaned_categorical_sample, tfidf_transformed_sample, test_size=0.3, random_state=95)\n",
    "\n",
    "XTrain.reindex()\n",
    "XTest.reindex();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of folds\n",
    "from sklearn.model_selection import KFold\n",
    "folds = 2\n",
    "kf = KFold(n_splits=folds)\n",
    "kf_inner = KFold(n_splits=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 7.15 µs\n",
      "0\n",
      "0 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, None, 0.01, None, 2.3076049815376423e+36)\n",
      "0 0.01 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-6c56abd8fbd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ma_inner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malpha_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_outer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mlasso_inner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_inner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfTrain_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouter_residuals_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mpredict_inner_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_inner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfTrain_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0minner_train_residuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_residuals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredict_inner_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, check_input)\u001b[0m\n\u001b[1;32m    750\u001b[0m                           \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m                           \u001b[0mselection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                           check_input=False)\n\u001b[0m\u001b[1;32m    753\u001b[0m             \u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_coef\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0mdual_gaps_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_dual_gap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_sparse_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m                 max_iter, tol, rng, random, positive)\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             model = cd_fast.enet_coordinate_descent_multi_task(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Test train and validate\n",
    "alpha_values = [0.01,0.1, 1, 10]\n",
    "from sklearn.linear_model import Lasso\n",
    "import copy\n",
    "\n",
    "mseOuterTrain = []\n",
    "mseOuterVal = []\n",
    "mseInnerTrain = []\n",
    "mseInnerVal = []\n",
    "mseOverallTrain = []\n",
    "mseOverallVal= []\n",
    "\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(yTrain)):\n",
    "    print(i)\n",
    "    yTrain_fold = yTrain[train_index]\n",
    "    yVal_fold = yTrain[val_index]\n",
    "\n",
    "    XTrain_fold = XTrain.iloc[train_index]\n",
    "    XVal_fold = XTrain.iloc[val_index]\n",
    "    \n",
    "\n",
    "    for a_outer in alpha_values:\n",
    "        print(i, a_outer)\n",
    "        lasso = Lasso(alpha=a_outer, random_state=111, max_iter=1000).fit(XTrain_fold, yTrain_fold)\n",
    "        predict_train = lasso.predict(XTrain_fold)\n",
    "        train_residuals = yTrain_fold - predict_train\n",
    "        mseOuterTrain.append(copy.deepcopy((i, None, a_outer ,None, np.square(train_residuals).mean())))\n",
    "        \n",
    "        predict_val = lasso.predict(XVal_fold)\n",
    "        val_residuals = yVal_fold - predict_val\n",
    "        mseOuterVal.append(copy.deepcopy((i, None, a_outer , None, np.square(val_residuals).mean())))\n",
    "        print((i, None, a_outer , None, np.square(val_residuals).mean()))\n",
    "        \n",
    "        outer_residuals = yTrain - lasso.predict(XTrain)\n",
    "            \n",
    "        for j, (train_inner_index, val_inner_index) in enumerate(kf_inner.split(tfidfTrain)):\n",
    "            tfidfTrain_fold = tfidfTrain[train_inner_index]\n",
    "            tfidfVal_fold = tfidfTrain[val_inner_index]\n",
    "            \n",
    "            outer_residuals_train = outer_residuals[train_inner_index]\n",
    "            outer_residuals_val = outer_residuals[val_inner_index]\n",
    "        \n",
    "            for a_inner in alpha_values:\n",
    "                print(j, a_outer, a_inner)\n",
    "                lasso_inner = Lasso(alpha=a_inner, random_state=111, max_iter=1000).fit(tfidfTrain_fold, outer_residuals_train)\n",
    "                predict_inner_train = lasso_inner.predict(tfidfTrain_fold)\n",
    "                inner_train_residuals = train_residuals - predict_inner_train\n",
    "                mseInnerTrain.append(copy.deepcopy((i, j, a_inner, np.square(inner_train_residuals).mean())))\n",
    "\n",
    "                predict_inner_val = lasso_inner.predict(tfidfVal_fold)\n",
    "                inner_val_residuals = val_residuals - predict_inner_val\n",
    "                mseInnerVal.append(copy.deepcopy((i, j, a_inner, np.square(inner_val_residuals).mean())))\n",
    "                print((i, j, a_inner, np.square(inner_val_residuals).mean()))\n",
    "            \n",
    "                residuals_overall_train = yTrain_fold - predict_train + predict_inner_train\n",
    "                mseOverallTrain.append(copy.deepcopy((i, j, a_outer, a_inner, np.square(residuals_overall_train).mean())))\n",
    "            \n",
    "                residuals_overall_val = yVal_fold - predict_val + predict_inner_val\n",
    "                mseOverallVal.append(copy.deepcopy((i, j, a_outer, a_inner, np.square(residuals_overall_val).mean())))\n",
    "                print((i, j, a_outer, a_inner, np.square(residuals_overall_val).mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted(mseOverallVal, key = operator.itemgetter(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Category (non-text) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.read_csv('Data/vocabulary.txt', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "#alpha_values = np.logspace(-4, 2, 6)\n",
    "alpha_values = [0.001,0.01,0.1]\n",
    "\n",
    "mseRow = []\n",
    "mseAll = []\n",
    "mseOuterTrain = []\n",
    "mseOuterValidate = []\n",
    "mseInner = []\n",
    "\n",
    "#manually grid search over alpha\n",
    "for alphaOut in alpha_values:\n",
    "    \n",
    "    #non-Text data\n",
    "    lasso = Lasso(alpha=alphaOut, random_state=111, max_iter=1000).fit(XTrain, yTrain)\n",
    "    yPredOut = lasso.predict(XValid)\n",
    "    residuals = yValid - yPredOut\n",
    "    \n",
    "    mseRow = []\n",
    "    \n",
    "    #can't figure out how to implement a validation or test accuracy as the training of the inner model is dependent \n",
    "    # on the outermodel's predictions.  If we predict on XValid, then we need to train the inner model on XValid.\n",
    "    # not sure if this makes sense\n",
    "    \n",
    "    #GridSearch manually\n",
    "    for alphaIn in alpha_values:\n",
    "        lassoTF = Lasso(alpha=alphaIn, random_state=777, max_iter=1000).fit(tfidfValid, residuals)\n",
    "        yPredIn = lassoTF.predict(tfidfValid)\n",
    "        \n",
    "        #add predictions from non-Text and Text regressions\n",
    "        yPredFull = yPredOut + yPredIn\n",
    "        \n",
    "        #calc mse between full predictions & actuals\n",
    "        mseRow.append(mean_squared_error(yValid, yPredFull))\n",
    "    \n",
    "    #store for of mse's\n",
    "    mseAll.append(mseRow)\n",
    "    \n",
    "mseAll = np.array(mseAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get bestAlphaOut & In based on the smallest mse from gridsearch\n",
    "bestAlpha_idx = np.unravel_index(np.argmin(mseAll), mseAll.shape)\n",
    "#bestAlpha_idx[0]\n",
    "bestAlphaOut = alpha_values[bestAlpha_idx[0]]\n",
    "bestAlphaIn = alpha_values[bestAlpha_idx[1]]\n",
    "print()\n",
    "print(\"Best Alpha for Outer Lasso (non-text): {}\".format(bestAlphaOut))\n",
    "print(\"Best Alpha for Linner Lasso (text): {}\".format(bestAlphaIn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run final model with bestAlphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=bestAlphaOut, random_state=111).fit(XTrain, yTrain)\n",
    "yPredOut = lasso.predict(XValid)\n",
    "residualsBest = yValid - yPredOut\n",
    "\n",
    "lassoTF = Lasso(alpha=bestAlphaIn, random_state=111).fit(tfidfTrain, residualsBest)\n",
    "yPredIn = lasso.predict(tfidfValid)\n",
    "\n",
    "yPredFull = yPredOut + yPredIn\n",
    "\n",
    "mse_Final = mean_squared_error(yValid, yPredFull)\n",
    "print(\"The MSE for the final tuned models is: {}\".format(mse_Final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(lasso, 'lasso_cat.pkl')\n",
    "joblib.dump(lassTF, 'lasso_tfidf.pkl')\n",
    "#clf = joblib.load('lasso_cat.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine non-Text & text data and run Lasso to see if prediction is diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine non-Text and Text to test it out\n",
    "# tfDTrain = tfidfTrain.todense()\n",
    "# comboTrain = pd.concat([XTrain.reset_index(), pd.DataFrame(tfDTrain)], axis=1)\n",
    "# comboTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save lasso on category data as DataFrame, write to csv\n",
    "# lasso_cat_coef = pd.DataFrame(lasso.coef_, index=XTrain.columns)\n",
    "# lasso_cat_coef.to_csv(\"lasso_cat_coef.csv\")\n",
    "\n",
    "# residualsDF = pd.DataFrame(residuals)\n",
    "# residualsDF.to_csv(\"residuals.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Text data on residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save lasso on TFIDF data as DataFrame, write to csv\n",
    "# lasso_tfidf_coef = pd.DataFrame(lassoTF.coef_, index=vocab.iloc[:,0].values)\n",
    "# lasso_tfidf_coef.to_csv(\"lasso_tfidf_coef.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Text data on residuals (per category1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the category1 column headers\n",
    "cat1_cols = XTrain.iloc[:,4815:4825].columns\n",
    "\n",
    "#reset index so can filter using list of indices\n",
    "residuals.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cat1_coefs = []\n",
    "tfidfTrain_df = pd.DataFrame(tfidfTrain.todense())  #I think can remove this and uncomment tocsr() line below\n",
    "\n",
    "for cat1 in cat1_cols:\n",
    "    #get indices where this category=1\n",
    "    idx = np.where(XTrain[cat1] == 1)[0]\n",
    "   \n",
    "    #filter data with the indices\n",
    "    X_tf = tfidfTrain_df.iloc[idx,:]   #Don't think need to use this, uncomment next time to deal with sparse\n",
    "    #X_tf = tfidfTrain.tocsr()[idx,:]  #way to filter sparse matrx with row indices\n",
    "    y_tf = residuals[idx]\n",
    "    \n",
    "    #run Lasso, fit, store coef array\n",
    "    lassoCat = Lasso(alpha=bestAlphaIn, random_state=10)\n",
    "    lassoCat.fit(X_tf, y_tf)\n",
    "\n",
    "    #add this cat1 coefs to a list\n",
    "    cat1_coefs.append(lassoCat.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#covert to DF with cols=vocab and index=category1 names\n",
    "lasso_allCat1_coef = pd.DataFrame(cat1_coefs, columns=vocab.iloc[:,0].values, index=cat1_cols)\n",
    "lasso_allCat1_coef.to_csv(\"lass_allCat1_coef.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><br>\n",
    "\n",
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
    "# from keras.layers.recurrent import LSTM, RNN, GRU, SimpleRNN\n",
    "# from keras.models import load_model\n",
    "# import keras\n",
    "# import h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #default layers = 6, neurons list always should equal 6\n",
    "# def build_NN(inShape, nnType='RNN'):\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(inShape[1], 64, input_length=maxLen))  #input layer\n",
    "#     if nnType == 'RNN':\n",
    "#         model.add(SimpleRNN(32, return_sequences=True))\n",
    "#         model.add(SimpleRNN(32))\n",
    "#         model.add(Dense(16, activation='relu'))\n",
    "#         model.add(Dense(8, activation='relu'))\n",
    "#     elif nnType == 'GRU':\n",
    "#         #model.add(Dense(64, input_shape=(inShape[1])))  #input layer\n",
    "#         model.add(GRU(32, return_sequences=True))\n",
    "#         model.add(GRU(32))\n",
    "#         model.add(Dense(16, activation='relu'))\n",
    "#         model.add(Dense(8, activation='relu'))\n",
    "#     elif nnType == 'LSTM':\n",
    "#         #model.add(Dense(64, input_shape=(inShape[1])))  #input layer\n",
    "#         model.add(LSTM(32, return_sequences=True)) \n",
    "#         model.add(LSTM(32))\n",
    "#         model.add(Dense(16, activation='relu'))      \n",
    "#         model.add(Dense(8, activation='relu'))\n",
    "#     elif nnType == 'BASE':\n",
    "#         #model.add(Dense(64, input_shape=(inShape[1],)))  #input layer\n",
    "#         model.add(Dense(32, activation='relu'))\n",
    "#         model.add(Dense(32, activation='relu'))\n",
    "#         model.add(Dense(16, activation='relu'))\n",
    "#         model.add(Reshape((-1,)))\n",
    "#         model.add(Dense(8, activation='relu'))\n",
    "#     else:\n",
    "#         print('should not be here')\n",
    "    \n",
    "#     model.add(Dense(1, activation='linear', kernel_initializer=\"uniform\")) #output layer\n",
    "#     model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "#     #model.summary() \n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #params for NN\n",
    "# epochs = 10\n",
    "# batchSize = 3000\n",
    "\n",
    "# #Need to prep the data for the NN\n",
    "\n",
    "# X = tokenizer.texts_to_sequences(train5k)\n",
    "# data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "\n",
    "# modelRNN = build_NN(tfidf_train.shape, nnType='RNN')\n",
    "# modelRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
